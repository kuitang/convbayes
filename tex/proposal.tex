\documentclass{article}
\usepackage[utf8]{inputenc}

\title{A Multimodal Topic Model of Deep Semantic Representations}
\author{Kui Tang and Sameer Lal}
\date{February 2015}

\begin{document}

\maketitle

\section{Introduction}

Our contributions include
\begin{enumerate}
\item A new topic model for documents containing text and images that works in a semantic representation of words and images, as opposed to a bag-of-words representation;
\item A technique to lift neural network parameters into latent variable models, enabling a coherent maximum-likelihood training via variational Bayes EM; and
\item State of the art performance on multimodal corpora, compared to existing multimodal topic models. 
\end{enumerate}

\section{Related Work}

\paragraph{Neural multimodal models.}
\citet{Lecun98} propose joining several neural network models for multimodal learning into a joint objective function, and then optimizing the parameters of each to maximize performance on this joint measure. \citet{Srivastava14} propose a joint text-image model using RBMs. Their model is fully generative, and the textual model is an undirected analogue to a topic model. [! To learn about undirected topic models and how they are different. Also to learn about the differences between RBMs and CNNs for vision, and think of why our approach is better!] However, their model relies on a binary semantic space, while ours is real.

\paragraph{Topic models on shallow image features.}
Several authors have applied topic modeling to visual or joint text and visual data, but none to date have utilized deep semantic representations as image features [! come up with the proper term of art for this. !].
\citet{Fritz08} trains LDA to detect object categories, but they use only gradient histogram representations.
Many authors have represented images as bag of words, which has been criticized (Citations and discussion in \citet{Cao07}).
[! Discuss visual words, and in particular, what the codebooks represent. !]
\citet{Cao07} improves on the image bag of words representation by modeling spatial constraints, which simultaneously learns to segment images. However, they use only a codebook of SIFT features as their image representations.
\citet{Barnard03} considers several models beginning with marginally independent emission distributions for words and image blobs (which are nevertheless conditional dependent) and proceeds to model additional dependencies between the distributions. Their image representations are non-adaptive color, position, geometry, and filter response features, endowed with Gaussian distributions.
\citet{Wang09} model alignment between words and image patches, but train on a supervised dataset and use only bag of codeword features to represent images, with codewords generated by $k$-means and SIFT.

\paragraph{Hybrid neural network and HMM models.}
A simpler problem related to our work is to combine output from a feedforward neural network with a Hidden Markov model. Several authors have proposed schemes tuned for a variety of tasks \citet{Trentin01}. [! Sameer, can you skim this paper and write a bit more about it? !] The approach closest to ours is that of \citet{Bengio92}, which derives gradient updates to a feedforward neural network based on the maximum-likelihood objective. Our work shares the same approach to optimize the neural network parameters to maximize the likelihood of the overall probabilistic model. [! TO UNDERSTAND: If their outputs $Y_t$ change, how do they properly model that as a fixed observation in the likelihood? I think our likelihood function must be a lot of general! ]. Instead of treating neural network \emph{output} as observed data, our model is more general because we lift \emph{parameters} from the neural network into latent variables in the overarching Bayesian network. We work with intractable Bayesian networks by using VB-EM.

\end{document}


